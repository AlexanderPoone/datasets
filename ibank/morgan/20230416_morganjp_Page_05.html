<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
	"http://www.w3.org/TR/1998/REC-html40-19980424/loose.dtd">
<html>
<head><meta http-equiv="content-type" content="text/html; charset=UTF-8">
<style type="text/css">
 table.main {}
 tr.row {}
 td.cell {}
 div.block {}
 div.paragraph {}
 .font0 { font:4pt Arial, sans-serif; }
 .font1 { font:6pt Arial, sans-serif; }
 .font2 { font:7pt Arial, sans-serif; }
 .font3 { font:9pt Arial, sans-serif; }
 .font4 { font:6pt Times New Roman, serif; }
 .font5 { font:12pt Times New Roman, serif; }

</style>
</head>
<body>
<p><span class="font5">J.P. Morgan</span></p>
<p><span class="font0">Albert Hung</span></p>
<p><span class="font0">(886-2)2725-9075</span></p>
<p><span class="font0">albert r&gt;ungg jpmcriai* com</span></p>
<p><span class="font0">A*la Pacific Equity Research</span></p>
<p><span class="font0">16Aprt2023</span></p>
<p><span class="font1">According to OpcnAI, the total computing workload used tn Al training has been doubling every 2 years starling from 1959. and the doubling time has accelerated to 3.4 months since 2012. Looking forward, the Research Lab believes that the doubling time is likely to speed up further given the increasing trend of algorithmic innovations (i.c. increasing genres of Al-specific chips) and improving cost burden (belter affordability of hardware chips for Al training).</span></p>
<p><span class="font1">Take generative large language model (LLM) training as an example. Total computing flops requirements have increased to 3.640 training petaflop's per day in GPT-3 I75B model (launched in 21120), - IOx more than 382 petaflop's in TS-I IB model (launched in2HI9).</span></p>
<div>
</div><br clear="all">
<div>
<p><span class="font4" style="font-weight:bold;">Floating pant operations per second (FLOPS) is a commonly used performance indicator of machine teaming hardware, doo to the prevalence of using floating point, instead of integer, in deep teaming.</span></p>
<p><span class="font4" style="font-weight:bold;">Of note. G.gaFLOPs= 10'9. T&lt;jraFLOPs= 10*12. PotaFLOPs- 10*15</span></p>
</div><br clear="all">
<p><span class="font1">How to translate required training parameters into GPU consumption?</span></p>
<p><span class="font1">Nvidia provides various computing power levels under different learning structures for a single GPU. Take A100 as an example. A single 4-GPU based DGX A100 server could generate 1.25 PetaFLOP per second under Tensor Float 32 structure. As the required total training compute of Chat GPT-3 is 3.14 • 10*23 FLOPS, this implies that it will take -300 4-GPU A100 server units to keep the training lime within 3 0 days, on our estimates.</span></p>
<p><span class="font1">Of note, the required training compute of each model is correlated with the number of parameters and training tokens, flic sharp increase in the parameters of new language models implies higher computing power consumption.</span></p>
<p><span class="font4" style="font-weight:bold;">5</span></p>
<div>
</div><br clear="all">
<div>
</div><br clear="all">
<div>
</div><br clear="all">
<div>
</div><br clear="all">
<div>
</div><br clear="all">
<div>
</div><br clear="all">
<div>
</div><br clear="all">
<div>
</div><br clear="all">
<div>
</div><br clear="all">
<p><span class="font3">This document is being provided for the exclusive use of </span><a href="mailto:anlhony.wc.liao@jpmorgan.com"><span class="font3">anlhony.wc.liao@jpmorgan.com</span></a><span class="font3"> &amp;&nbsp;clients of J.P. Morgan.</span></p>
</body>
</html>